---
aliases: /news/q-a-why-mental-health-chatbots-need-strict-safety-guardrails
archetype: curated-content
author:
  - Athira Ravi
basePath: /news/
breadcrumbLinks:
  - /
  - /news/
  - ''
breadcrumbs:
  - Home
  - News
  - 'Q&A: Why mental health chatbots need strict safety guardrails'
categories:
  - 'Gartner: Healthcare'
  - 'Medigy: Mental Health'
categorySlug:
  - 'gartner: healthcare'
  - 'medigy: mental health'
categoryUrl:
  - topic/gartner-healthcare
  - topic/medigy-mental-health
categoryLabel:
  - Healthcare
  - Mental Health
contentCategories: netspective-medigy-news-curated-content
institution: null
offering: null
layOut: single
date: '2022-08-22'
description: >-
  Ramakant Vempati, Wysa cofounder and president, discusses how the startup
  tests its AI-backed chatbot to monitor safety and quality. Wysa, maker of an
  AI-backed chatbot that aims to help users work th
favIconImage: null
featuredImage:
  alt: 'Q&A: Why mental health chatbots need strict safety guardrails'
  format: JPEG
  href: f754701a-8ab3-587e-813e-f4828abf6a6a-featuredImage.jpeg
  size:
    - 630
    - 1200
  valid: true
  workPackage: 14545
  wpAttachment:
    fileName: Curated_Featured_Image.jpg
    link: /api/v3/attachments/26783/content
featuredPdf: null
htmlMetaData:
  author: null
  description: null
  generator: null
  viewport: null
  articlemodified_time: null
  articlepublished_time: null
  msvalidate.01: null
  ogdescription: >-
    <p>Ramakant Vempati, Wysa cofounder and president, discusses how the startup
    tests its AI-backed chatbot to monitor safety and quality.&nbsp;</p>
  ogimage: null
  ogsite_name: null
  ogtitle: 'Q&A: Why mental health chatbots need strict safety guardrails'
  ogtype: null
  ogupdated_time: null
  ogurl: null
  yandex-verification: null
  robots: null
  fbapp_id: null
  oglocale: null
  fbadmins: null
  articlepublisher: null
  google-site-verification: null
  keywords: null
id: 14545
identifier: News
lastMod: '2022-08-22T11:39:11.654948Z'
link:
  brand: mobihealthnews.com
  href: >-
    https://www.mobihealthnews.com/news/qa-why-mental-health-chatbots-need-strict-safety-guardrails
  original: >-
    https://www.mobihealthnews.com/news/qa-why-mental-health-chatbots-need-strict-safety-guardrails
href: >-
  https://www.mobihealthnews.com/news/qa-why-mental-health-chatbots-need-strict-safety-guardrails
original: >-
  https://www.mobihealthnews.com/news/qa-why-mental-health-chatbots-need-strict-safety-guardrails
mastHead: NEWS
mdName: f754701a-8ab3-587e-813e-f4828abf6a6a.md
openGraphMetaData:
  ogdescription: >-
    <p>Ramakant Vempati, Wysa cofounder and president, discusses how the startup
    tests its AI-backed chatbot to monitor safety and quality.&nbsp;</p>
  ogtitle: 'Q&A: Why mental health chatbots need strict safety guardrails'
  ogurl: null
  ogimage: null
  ogsite_name: null
  ogtype: null
  ogupdated_time: null
  ogimageheight: null
openProjectCustomFields:
  cleanUrl: >-
    https://www.mobihealthnews.com/news/qa-why-mental-health-chatbots-need-strict-safety-guardrails
  medigyTopics:
    - 'Gartner: Healthcare'
    - 'Medigy: Mental Health'
  sourceUrl: >-
    https://www.mobihealthnews.com/news/qa-why-mental-health-chatbots-need-strict-safety-guardrails
openProjectWorkPackageType: Curated Content
searchCategory: News
slug: mobihealthnews-q-a-why-mental-health-chatbots-need-strict-safety-guardrails
source: open-project-curations
status: ''
sub: brief
tags:
  - News
title: 'Q&A: Why mental health chatbots need strict safety guardrails'
via: ' '
insights_description: null
insights_name: null
viaLink: null
relatedOfferings:
  - label: Sessions - Electronic Health Record
    permalink: /offering/sessions---electronic-health-record
    categories:
      - 'Symplur: EHR (Electronic Health Record)'
      - 'Medigy: Mental Health'
    offeringId: 18203
  - label: GrayMatters Prism
    permalink: /offering/graymatters-prism
    categories:
      - 'Symplur: Digital Therapeutics'
      - 'Medigy: Mental Health'
    offeringId: 17295
  - label: Emulait Starter Kit
    permalink: /offering/emulait-starter-kit
    categories:
      - 'Gartner: Healthcare'
    offeringId: 17281
  - label: DeepIntent Platform
    permalink: /offering/deepintent-platform
    categories:
      - 'Gartner: Healthcare'
    offeringId: 17259
  - label: Aiberry's Solution
    permalink: /offering/aiberrys-solution
    categories:
      - 'Medigy: Mental Health'
    offeringId: 17133
twitterMetaData:
  twittercard: null
  twitterdescription: >-
    <p>Ramakant Vempati, Wysa cofounder and president, discusses how the startup
    tests its AI-backed chatbot to monitor safety and quality.&nbsp;</p>
  twittertitle: 'Q&A: Why mental health chatbots need strict safety guardrails'
  twitterimage: null
  twitterurl: null
---
Ramakant Vempati, Wysa cofounder and president, discusses how the startup tests its AI-backed chatbot to monitor safety and quality. Wysa, maker of an AI-backed chatbot that aims to help users work though concerns like anxiety, stress and low mood, recently announced a $20 million Series B funding raise, not long after the startup received FDA Breakthrough Device Designation to use its tool to help adults with chronic musculoskeletal pain. Ramakant Vempati, the company&#39;s cofounder and president, sat down with MobiHealthNews to discuss how the chatbot works, the guardrails Wysa uses to monitor safety and quality, and what&#39;s next after its latest funding round.

From a product point of view, users may or may not think about it directly, but the safety and the guardrails which we built into the product to make sure that it&#39;s fit for purpose in that wellness context is an essential part of the value we provide. When we went live in 2017, I was like, &quot;Will people really talk to a chatbot about their deepest, darkest fears?&quot; You use chatbots in a customer service context, like a bank website, and frankly, the experience leaves much to be desired. I think phase one has been proving to ourselves, really convincing ourselves, that users like it and they derive value out of the service. I think phase two has been to prove this in terms of clinical outcomes. Where we use NLP \[natural language processing\], we are using NLU, natural language understanding, to understand user context and to understand what they&#39;re talking about and what they&#39;re looking for. There will always be instances where people say something ambiguous, or they will use nested or complicated sentences, and the AI models will not be able to catch them. And we comply with a safety standard used by the NHS in the U.K. We have a large clinical safety data set, which we use because we&#39;ve now had 500 million conversations on the platform. Every time we create a new conversation script, we then test with this data set. Vempati: In the early days of Wysa, we used to have people writing in, volunteering to translate. So, it&#39;s a combination of market feedback and strategic priorities, as well as what the product can handle, places where it is easier to use AI in that particular language with clinical safety.